"""Unit tests for metrics collection infrastructure."""

import threading
import time
from unittest.mock import patch

from cachekit.reliability.metrics_collection import (
    AsyncMetricsCollector,
    PrometheusMetricsRegistry,
    get_async_metrics_collector,
    record_async_metric,
)


class TestPrometheusMetricsRegistry:
    """Test Prometheus metrics registry functionality."""

    def test_get_or_create_metric_creates_new_metric(self):
        """Test that new metrics are created successfully."""
        from prometheus_client import Counter

        metric = PrometheusMetricsRegistry.get_or_create_metric(
            Counter, "test_counter_new", "Test counter", ["label1", "label2"]
        )

        assert metric is not None
        assert metric._name == "test_counter_new"

    def test_get_or_create_metric_returns_existing_metric(self):
        """Test that existing metrics are returned instead of creating duplicates."""
        from prometheus_client import Counter

        # Create first metric
        metric1 = PrometheusMetricsRegistry.get_or_create_metric(Counter, "test_counter_existing", "Test counter", ["label1"])

        # Try to create same metric again
        metric2 = PrometheusMetricsRegistry.get_or_create_metric(Counter, "test_counter_existing", "Test counter", ["label1"])

        # Should return the same instance
        assert metric1 is metric2

    def test_get_or_create_metric_without_labels(self):
        """Test metric creation without labels."""
        from prometheus_client import Gauge

        metric = PrometheusMetricsRegistry.get_or_create_metric(Gauge, "test_gauge_no_labels", "Test gauge without labels")

        assert metric is not None
        assert metric._name == "test_gauge_no_labels"


class TestAsyncMetricsCollector:
    """Test async metrics collection functionality."""

    def test_collector_initialization(self):
        """Test that collector initializes properly."""
        collector = AsyncMetricsCollector(max_queue_size=100, worker_timeout=0.5)

        assert collector.max_queue_size == 100
        assert collector.worker_timeout == 0.5
        assert collector._worker_thread is not None
        assert collector._worker_thread.is_alive()

        # Clean up
        collector.shutdown()

    def test_record_counter_metric(self):
        """Test recording counter metrics."""
        collector = AsyncMetricsCollector()

        # Record some counter metrics
        collector.record_counter("test_counter", {"label1": "value1"}, 2.0)
        collector.record_counter("test_counter", {"label1": "value2"}, 1.0)

        # Allow time for processing
        time.sleep(0.1)

        stats = collector.get_stats()
        assert stats["processed_count"] >= 2
        assert stats["dropped_count"] == 0

        collector.shutdown()

    def test_record_histogram_metric(self):
        """Test recording histogram metrics."""
        collector = AsyncMetricsCollector()

        # Record some histogram metrics
        collector.record_histogram("test_histogram", 0.025, {"operation": "get"})
        collector.record_histogram("test_histogram", 0.15, {"operation": "set"})

        # Allow time for processing
        time.sleep(0.1)

        stats = collector.get_stats()
        assert stats["processed_count"] >= 2

        collector.shutdown()

    def test_record_gauge_metric(self):
        """Test recording gauge metrics."""
        collector = AsyncMetricsCollector()

        # Record some gauge metrics
        collector.record_gauge("test_gauge", 0.75, {"type": "utilization"})
        collector.record_gauge("test_gauge", 0.85, {"type": "utilization"})

        # Allow time for processing
        time.sleep(0.1)

        stats = collector.get_stats()
        assert stats["processed_count"] >= 2

        collector.shutdown()

    def test_queue_overflow_handling(self):
        """Test that metrics are dropped when queue is full."""
        # Create collector with very small queue
        collector = AsyncMetricsCollector(max_queue_size=2)

        # Flood with metrics to trigger overflow
        for i in range(20):
            collector.record_counter("overflow_test", {"id": str(i)})

        # Allow time for processing
        time.sleep(0.2)

        stats = collector.get_stats()
        # Some metrics should have been dropped
        assert stats["dropped_count"] > 0

        collector.shutdown()

    def test_worker_thread_restart(self):
        """Test that worker thread can be restarted if it dies."""
        collector = AsyncMetricsCollector()

        # Stop the worker thread
        collector._shutdown_event.set()
        collector._worker_thread.join(timeout=1.0)

        # Restart the worker
        collector._shutdown_event.clear()
        collector._start_worker()

        assert collector._worker_thread.is_alive()

        collector.shutdown()

    def test_flush_functionality(self):
        """Test flushing the metrics queue."""
        collector = AsyncMetricsCollector()

        # Add some metrics
        for i in range(5):
            collector.record_counter("flush_test", {"id": str(i)})

        # Flush and ensure all are processed
        collector.flush(timeout=2.0)

        stats = collector.get_stats()
        assert stats["queue_size"] == 0
        assert stats["processed_count"] >= 5

        collector.shutdown()

    def test_graceful_shutdown(self):
        """Test graceful shutdown of collector."""
        collector = AsyncMetricsCollector()

        # Add some metrics
        for i in range(3):
            collector.record_counter("shutdown_test", {"id": str(i)})

        initial_stats = collector.get_stats()

        # Shutdown gracefully
        collector.shutdown(timeout=2.0)

        # Worker thread should be stopped
        assert not collector._worker_thread.is_alive()

        # Stats should still be available
        final_stats = collector.get_stats()
        assert final_stats["processed_count"] >= initial_stats["processed_count"]

    def test_concurrent_metric_recording(self):
        """Test thread safety of metric recording."""
        collector = AsyncMetricsCollector()

        def record_metrics(thread_id: int):
            for i in range(10):
                collector.record_counter("concurrent_test", {"thread": str(thread_id), "metric": str(i)})

        # Start multiple threads recording metrics
        threads = []
        for thread_id in range(5):
            thread = threading.Thread(target=record_metrics, args=(thread_id,))
            threads.append(thread)
            thread.start()

        # Wait for all threads to complete
        for thread in threads:
            thread.join()

        # Allow time for processing
        time.sleep(0.2)

        stats = collector.get_stats()
        # Should have processed 50 metrics (5 threads Ã— 10 metrics each)
        assert stats["processed_count"] >= 50
        assert stats["dropped_count"] == 0

        collector.shutdown()

    def test_error_handling_in_worker_thread(self):
        """Test that worker thread handles errors gracefully."""
        collector = AsyncMetricsCollector()

        # Create a malformed metric that will cause processing error
        malformed_metric = {"type": "unknown", "name": None, "value": "invalid"}
        collector._enqueue_metric(malformed_metric)

        # Record a valid metric after the malformed one
        collector.record_counter("valid_metric", {"test": "value"})

        # Allow time for processing
        time.sleep(0.2)

        # Worker should still be alive despite the error
        assert collector._worker_thread.is_alive()

        # Valid metric should still be processed
        stats = collector.get_stats()
        assert stats["processed_count"] >= 1

        collector.shutdown()


class TestGlobalMetricsCollector:
    """Test global metrics collector functionality."""

    def test_get_async_metrics_collector_singleton(self):
        """Test that global collector returns singleton instance."""
        collector1 = get_async_metrics_collector()
        collector2 = get_async_metrics_collector()

        assert collector1 is collector2
        assert collector1._worker_thread.is_alive()

    def test_record_async_metric_convenience_function(self):
        """Test the convenience function for recording metrics."""
        # Test different metric types
        record_async_metric("counter", "test_async_counter", 1.0, {"test": "label"})
        record_async_metric("histogram", "test_async_histogram", 0.5, {"test": "label"})
        record_async_metric("gauge", "test_async_gauge", 0.75, {"test": "label"})

        # Test unknown metric type
        with patch("cachekit.reliability.metrics_collection.logger") as mock_logger:
            record_async_metric("unknown", "test_unknown", 1.0)
            mock_logger.warning.assert_called_once()

    def test_worker_thread_auto_restart(self):
        """Test that global collector auto-restarts worker thread."""
        collector = get_async_metrics_collector()

        # Kill the worker thread
        collector._shutdown_event.set()
        collector._worker_thread.join(timeout=1.0)

        # Get collector again - should restart worker
        collector = get_async_metrics_collector()

        # The restart functionality is verified by the warning log message
        # that appears when the function detects a dead worker thread
        assert collector is not None  # Basic verification that we got a collector


class TestMetricsIntegration:
    """Test integration with Prometheus metrics."""

    @patch("cachekit.reliability.metrics_collection.cache_operations")
    def test_prometheus_counter_integration(self, mock_counter):
        """Test integration with Prometheus counter metrics."""
        collector = AsyncMetricsCollector()

        # Record a cache operation metric
        collector.record_counter("redis_cache_operations_total", {"operation": "get", "status": "hit"}, 1.0)

        # Allow time for processing
        time.sleep(0.1)

        # Should have called the Prometheus metric
        mock_counter.labels.assert_called_with(operation="get", status="hit")
        mock_counter.labels().inc.assert_called_with(1.0)

        collector.shutdown()

    @patch("cachekit.reliability.metrics_collection.cache_latency")
    def test_prometheus_histogram_integration(self, mock_histogram):
        """Test integration with Prometheus histogram metrics."""
        collector = AsyncMetricsCollector()

        # Record a latency metric
        collector.record_histogram("redis_cache_operation_duration_seconds", 0.025, {"operation": "set"})

        # Allow time for processing
        time.sleep(0.1)

        # Should have called the Prometheus metric
        mock_histogram.labels.assert_called_with(operation="set")
        mock_histogram.labels().observe.assert_called_with(0.025)

        collector.shutdown()

    @patch("cachekit.reliability.metrics_collection.circuit_breaker_state")
    def test_prometheus_gauge_integration(self, mock_gauge):
        """Test integration with Prometheus gauge metrics."""
        collector = AsyncMetricsCollector()

        # Record a circuit breaker state metric
        collector.record_gauge("redis_circuit_breaker_state", 1.0, {"namespace": "test"})

        # Allow time for processing
        time.sleep(0.1)

        # Should have called the Prometheus metric
        mock_gauge.labels.assert_called_with(namespace="test")
        mock_gauge.labels().set.assert_called_with(1.0)

        collector.shutdown()

    def test_unknown_metric_name_handling(self):
        """Test handling of unknown metric names."""
        collector = AsyncMetricsCollector()

        with patch("cachekit.reliability.metrics_collection.logger") as mock_logger:
            # Record a metric with unknown name
            collector.record_counter("unknown_metric_name", {"test": "value"}, 1.0)

            # Allow time for processing
            time.sleep(0.1)

            # Should log debug message about unknown metric
            mock_logger.debug.assert_called()
